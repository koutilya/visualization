{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from difflib import SequenceMatcher\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/sanskrit_transcoder')\n",
    "# import transcoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"module.name\",\"sanskrit_transcoder/transcoder.py\")\n",
    "transcoder = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(transcoder)\n",
    "transcoder.transcoder_set_dir('sanskrit_transcoder/data/transcoder');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sandhi implemented here .\n",
    "names = [\"workdata/lines_submit/correct_lines/sorteddata/\",\"workdata/lines_submit/wrong_lines/sorteddata/\"]\n",
    "\n",
    "# fnames = os.listdir(\"workdata/lines_submit/correct_lines/sorteddata/\")\n",
    "for name in names :\n",
    "    fnames = os.listdir(name)\n",
    "    for fname in fnames:\n",
    "        if fname == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "#         print(fname+\"/dataframe.txt\")\n",
    "        with open(name+fname+\"/input_line.txt\",encoding = 'utf-8' ,mode ='r') as fr :\n",
    "            lines = fr.readlines()\n",
    "        for line in lines :\n",
    "            if(line.startswith(\"converted line : \")) :\n",
    "                line1 = line[17:]\n",
    "                break\n",
    "        chunkwords = line1.split(\"\\xa0\")\n",
    "#         print(chunkwords)\n",
    "        df = pd.read_csv(name+fname+\"/dataframe.txt\",encoding = 'utf-8' ,sep = ',')\n",
    "        cf = pd.read_csv(name+fname+\"/conflicts.csv\",encoding = 'utf-8' ,sep = ',')\n",
    "        df['word_slp1'] = df['word']\n",
    "        for i in df.index:\n",
    "            df.loc[i,'word_slp1']   =  transcoder.transcoder_processString(df.loc[i,'word'],'roman','slp1')  ##transcoder\n",
    "        df['sandhi_indexs'] = \"\"\n",
    "        df['sandhi_words'] = \"\"\n",
    "        for i in df.index : \n",
    "            word1 = df.loc[i,'word_slp1']\n",
    "            chunkno = df.loc[i,'chunk_no']\n",
    "            position1 =  df.loc[i,'position']\n",
    "            level = df.loc[i,'level']\n",
    "            color1 = df.loc[i,'color_class']\n",
    "            clspan1 = df.loc[i,'colspan']\n",
    "            chunkword = transcoder.transcoder_processString(chunkwords[chunkno-1],'roman','slp1')   ## transcoder\n",
    "            df1 = df.loc[(df['chunk_no'] == chunkno) & (df['level']!=level)]\n",
    "            for j in df1.index :\n",
    "                if (cf.loc[i,str(j)] != 1) and (cf.loc[i,str(j)] != 0):\n",
    "                    cf.loc[i,str(j)] = 2\n",
    "                position2 = df1.loc[j,'position']\n",
    "                color2 = df.loc[i,'color_class']\n",
    "                word2 = df1.loc[j,'word_slp1']\n",
    "                word3 = df1.loc[j,'word']\n",
    "                if(position2 == position1 + clspan1-1) and (clspan1!=1):\n",
    "                    if((color1=='yellow_back') and (color2=='carmin_back'))  :    ## check color\n",
    "                        continue\n",
    "                    if((color2=='yellow_back') and (color1=='carmin_back'))  :    ## check color\n",
    "                        continue\n",
    "                    if(len(s.loc[(s['c1']==word1[-1:] )& (s['c2']==word2[:1])])):\n",
    "                        \n",
    "                        x=s.loc[(s['c1']==word1[-1:] )& (s['c2']==word2[:1])]\n",
    "                        for c3 in x['c3']:\n",
    "                            mergeword = word1[:-1] + c3 + word2[1:]\n",
    "#                             print(mergeword+\"---\"+chunkword)\n",
    "                            if(re.findall(mergeword,chunkword)):               ## checking  mergedword(spl1) in chunkword\n",
    "                                df.loc[i,'sandhi_indexs'] += str(j)+\", \"\n",
    "                                df.loc[i,'sandhi_words'] += word3+\"(\"+word2+\"), \"\n",
    "                                cf.loc[i,str(j)] = 1                             ## merge pssible so change conflict\n",
    "                                cf.loc[j,str(i)] = 1   \n",
    "#                                 print(cf.loc[i,str(j)])\n",
    "                                break\n",
    "                    elif(word1[-1:]=='H'):\n",
    "                        if(len(s.loc[(s['c1']==word1[-2:] )& (s['c2']==word2[:1])])):\n",
    "                        \n",
    "                            x=s.loc[(s['c1']==word1[-2:] )& (s['c2']==word2[:1])]\n",
    "                            for c3 in x['c3']:\n",
    "                                mergeword = word1[:-1] + c3 + word2[1:]\n",
    "                                if(re.findall(mergeword,chunkword)):          ## checking  mergedword(spl1) in chunkword   \n",
    "                                    df.loc[i,'sandhi_indexs'] += str(j)+\", \"\n",
    "                                    df.loc[i,'sandhi_words'] += word3+\"(\"+word2+\"), \"\n",
    "                                    cf.loc[i,str(j)] = 1                        ## merge pssible so change conflict\n",
    "                                    cf.loc[j,str(i)] = 1 \n",
    "                                    break\n",
    "                elif(position2 > position1 + clspan1-1) :\n",
    "                    cf.loc[i,str(j)] = 0\n",
    "                    cf.loc[j,str(i)] = 0\n",
    "        \n",
    "        df.to_csv(name+fname+\"/dataframe_withsandhi.txt\" ,encoding = 'utf-8' , sep=',',index = False,mode = 'w' )\n",
    "        \n",
    "       ## for conflict finding 0 = no conflict,1=possible merge,2=conflict \n",
    "        for i in df.index :\n",
    "            chunkno = df.loc[i,'chunk_no']\n",
    "            position1 =  df.loc[i,'position']\n",
    "            level = df.loc[i,'level']\n",
    "            color1 = df.loc[i,'color_class']\n",
    "            clspan1 = df.loc[i,'colspan']\n",
    "            df1 = df.loc[(df['chunk_no'] != chunkno)]\n",
    "            for j in df1.index :\n",
    "                cf.loc[i,str(j)] = 0\n",
    "            df1 = df.loc[(df['level']==level) & (df['chunk_no'] == chunkno)]\n",
    "            for j in df1.index :\n",
    "                cf.loc[i,str(j)] = 0\n",
    "\n",
    "        cf.to_csv(name+fname+\"/conflicts.csv\" ,encoding = 'utf-8' , sep=',',index = False,mode = 'w' )               \n",
    "                        \n",
    "#         break\n",
    "#     break\n",
    "# print(name+fname+\"/dataframe_withsandhi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting undefined words,without conflict sentences for partial,complete analasys\n",
    "\n",
    "names = [\"workdata/lines_submit/correct_lines/sorteddata/\",\"workdata/lines_submit/wrong_lines/sorteddata/\"]\n",
    "\n",
    "os.makedirs('workdata/lines_submit/analasys/', exist_ok=True)\n",
    "\n",
    "pbwords = []\n",
    "\n",
    "clinefolder = []\n",
    "wlinefolder = []\n",
    "                  \n",
    "for name in names : \n",
    "    fnames = os.listdir(name)\n",
    "    for fname in fnames:\n",
    "        if fname == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        if(name == names[1]):\n",
    "            \n",
    "            with open(name+fname+\"/problem_words.txt\",encoding = 'utf-8' ,mode ='r') as fr :\n",
    "                c =1\n",
    "                for line in fr.readlines() :\n",
    "                    if c==1 :\n",
    "                        c=0\n",
    "                        continue\n",
    "#                     if not line[:-1] in pbwords :\n",
    "                    pbwords.append(line[:-1])\n",
    "        \n",
    "        cf = pd.read_csv(name+fname+\"/conflicts.csv\",encoding = 'utf-8' ,sep = ',')\n",
    "        bl = 0\n",
    "        for i in range(0,len(cf)):\n",
    "            for j in range(0,len(cf)):\n",
    "                if not cf.loc[i,str(j)]==0:\n",
    "                    bl=1\n",
    "        if(bl == 0):\n",
    "            if(name==names[0]):\n",
    "                clinefolder.append(name+fname)\n",
    "            else :\n",
    "                wlinefolder.append(name+fname)\n",
    "    if(name==names[0]):\n",
    "        with open(\"workdata/lines_submit/analasys/correctlines_noconflics_folders.txt\",encoding = 'utf-8' ,mode ='w') as fw :\n",
    "            fw.writelines(\"\\n\".join(clinefolder))\n",
    "    else :\n",
    "        with open(\"workdata/lines_submit/analasys/wronglines_noconflics_folders.txt\",encoding = 'utf-8' ,mode ='w') as fw :\n",
    "            fw.writelines(\"\\n\".join(wlinefolder))\n",
    "os.makedirs('workdata/lines_submit/analasys/', exist_ok=True)\n",
    "with open(\"workdata/lines_submit/analasys/undifined_words.txt\",encoding = 'utf-8' ,mode ='w') as fw :\n",
    "    fw.writelines(\"\\n\".join(pbwords))\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"workdata/lines_submit/analasys/undifined_words.txt\",encoding = 'utf-8' ,mode ='r') as fr :\n",
    "    pbwords = fr.readlines()\n",
    "pf = pd.DataFrame(index=range(0,1))\n",
    "for pbw in pbwords:\n",
    "    pf[pbw[:-1]] = 0\n",
    "for pbw in pbwords:\n",
    "    pf[pbw[:-1]] += 1\n",
    "pf.to_csv(\"workdata/lines_submit/analasys/undifined_wordsfreaquency.csv\",encoding = 'utf-8' , sep=',',index = False,mode = 'w' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pbwords\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names :\n",
    "    fnames = os.listdir(name)\n",
    "    for fname in fnames:\n",
    "        if fname == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        df1 = pd.read_csv(name+fname+\"/dataframe.txt\",encoding = 'utf-8' ,sep = ',')\n",
    "        df2 = df1 = pd.read_csv(name+fname+\"/dataframe_withsandhi.txt\",encoding = 'utf-8' ,sep = ',')\n",
    "        df1.to_csv(name+fname+\"/dataframe.csv\" ,encoding = 'utf-8' , sep=',',index = False,mode = 'w' )\n",
    "        df2.to_csv(name+fname+\"/dataframe_withsandhi.csv\" ,encoding = 'utf-8' , sep=',',index = False,mode = 'w' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
